# lstm_bpm.py - BPM íšŒê·€ ëª¨ë¸ ì‹œìŠ¤í…œ
import os
import sys
# ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì–´ë–¤ ìœ„ì¹˜ì—ì„œ ì‹¤í–‰í•˜ë”ë¼ë„ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ import ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
# CuBLAS ê²°ì •ì  ë™ì‘ì„ ìœ„í•´ í™˜ê²½ë³€ìˆ˜ ì„¤ì • (torch/CUDA ì´ˆê¸°í™” ì´ì „)
os.environ.setdefault("CUBLAS_WORKSPACE_CONFIG", ":4096:8")
import numpy as np
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
from helpers.preproc_lstm import (
    extract_phase_derivative,
)
from helpers.preproc_signal import range_axis_m
from helpers.radar_config import FS_ADC, PAD_FT, B_HZ, NUM_SAMPLES
from typing import Tuple, List, Optional, Dict

# ===== í•™ìŠµ íŒŒë¼ë¯¸í„° =====
EPOCHS = 1000                   # ì—í¬í¬
LEARNING_RATE = 1e-4          # ì§ì ‘ BPM ì˜ˆì¸¡ìš© ë‚®ì€ í•™ìŠµë¥ 
HIDDEN_DIM = 256
NUM_LAYERS = 1                # LSTM ë ˆì´ì–´ 2ì¸µ ë° ë“œë¡­ì•„ì›ƒ ì ìš©

VALIDATION_SPLIT = 0.25       # ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (20%ë¡œ ì¤„ì„)
EARLY_STOP_PATIENCE = 100      # ì–¼ë¦¬ ìŠ¤íƒ‘ ì¸ë‚´ì‹¬ (ì—í¬í¬) - ë” ì—¬ìœ ë¡­ê²Œ
EARLY_STOP_MIN_DELTA = 1e-5   # ìµœì†Œ ê°œì„  ì„ê³„ê°’ - ë” ê´€ëŒ€í•˜ê²Œ

# ===== ìŠ¤ì¼€ì¤„ëŸ¬ íŒŒë¼ë¯¸í„° =====
SCHEDULER_FACTOR = 0.5        # í•™ìŠµë¥  ê°ì†Œ ë¹„ìœ¨
SCHEDULER_PATIENCE = 60       # ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ë‚´ì‹¬ (ì—í¬í¬)
SCHEDULER_MIN_LR = 1e-5       # ìµœì†Œ í•™ìŠµë¥ 

# ===== ì‹ í˜¸ ì²˜ë¦¬ íŒŒë¼ë¯¸í„° =====
FS          = 36.0            # í”„ë ˆì„ë ˆì´íŠ¸ (Hz)
WIN_FRAMES  = int(8.0 * FS)   # 8ì´ˆ ìœˆë„ìš° = 288 í”„ë ˆì„
HOP_FRAMES  = 18*2            # 1ì´ˆ í™‰ 18*2í”„ë ˆì„
# FMIN, FMAX  = 0.5, 3.33       # ì‹¬ë°• ëŒ€ì—­ [Hz] (30-200 BPMì— ëŒ€ì‘)
# PAD_FACTOR  = 8               # FFT íŒ¨ë”© (ì£¼íŒŒìˆ˜ í•´ìƒë„ í–¥ìƒ)
FEATURE_DIM = 36              # 1D CNNìœ¼ë¡œ ì••ì¶•í•  íŠ¹ì§• ì°¨ì›

# ===== ê²½ë¡œ ì„¤ì • =====
TRAIN_DATA_DIR = "record2/train/data/"
TRAIN_ANSWER_DIR = "record2/train/answer/"
TEST_DATA_DIR = "record2/test/data/"
TEST_ANSWER_DIR = "record2/test/answer/"

# ===== ì‹œë“œ ê³ ì • (ì¬í˜„ì„±) =====
seed = 42
os.environ["PYTHONHASHSEED"] = str(seed)
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
try:
    torch.use_deterministic_algorithms(True)
except Exception:
    pass
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# ===== íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ =====

def calculation(file_path: str) -> Tuple[int, np.ndarray]:
    """ë ˆì´ë” ë°ì´í„°ì—ì„œ ìµœì  ê±°ë¦¬ binê³¼ ìœ„ìƒ ì‹ í˜¸ ì¶”ì¶œ (ì „ì²˜ë¦¬ëœ ë°ì´í„° ë˜ëŠ” ì›ë³¸ ë°ì´í„° ëª¨ë‘ ì§€ì›)"""
    data = np.load(file_path, allow_pickle=True)
    
    # ìƒˆë¡œìš´ ì „ì²˜ë¦¬ëœ ë°ì´í„° í˜•ì‹ì¸ì§€ í™•ì¸
    if isinstance(data, np.ndarray) and data.ndim == 0 and isinstance(data.item(), dict):
        # ì „ì²˜ë¦¬ëœ ë°ì´í„° (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)
        processed_dict = data.item()
        fc_bin = processed_dict['fc_bin']
        z_tau = processed_dict['z_tau']
        
        # ê±°ë¦¬ ì¶• ê³„ì‚° í›„ fc_binì˜ ê±°ë¦¬[m] í‘œê¸°
        rng_axis, _, _ = range_axis_m(FS_ADC, NUM_SAMPLES, PAD_FT, B_HZ)
        dist_m = float(rng_axis[int(fc_bin)]) if 0 <= int(fc_bin) < len(rng_axis) else float('nan')
        print(f"[CALCULATION] ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ: fc_bin={fc_bin} ({dist_m:.2f} m)")
        
        return int(fc_bin), z_tau
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹: type={type(data)}, shape={getattr(data, 'shape', 'N/A')}")

def find_matching_files(data_dir: str, answer_dir: str) -> List[Tuple[str, str]]:
    """í´ë”ì—ì„œ ë§¤ì¹­ë˜ëŠ” ë°ì´í„°-ì •ë‹µ íŒŒì¼ ìŒì„ ì°¾ê¸°"""
    if not os.path.exists(data_dir) or not os.path.exists(answer_dir):
        return []
    
    data_files = [f for f in os.listdir(data_dir) if f.endswith('.npy')]
    answer_files = [f for f in os.listdir(answer_dir) if f.endswith('.csv')]
    
    # ë²ˆí˜¸ë¡œ ë§¤ì¹­
    pairs = []
    for data_file in data_files:
        # íŒŒì¼ëª…ì—ì„œ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: "5.npy" -> "5")
        try:
            data_num = os.path.splitext(data_file)[0]
            answer_file = f"{data_num}.csv"
            
            if answer_file in answer_files:
                data_path = os.path.join(data_dir, data_file)
                answer_path = os.path.join(answer_dir, answer_file)
                pairs.append((data_path, answer_path))
        except Exception:
            continue
    
    # ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬
    pairs.sort(key=lambda x: int(os.path.splitext(os.path.basename(x[0]))[0]))
    return pairs

def load_ground_truth(path: str) -> Optional[np.ndarray]:
    """ECG ì •ë‹µ íŒŒì¼ ë¡œë“œ (CSV í˜•ì‹)"""
    if not os.path.exists(path):
        return None
    timestamps: List[float] = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            # CSV í—¤ë” ê±´ë„ˆë›°ê¸°
            if line.startswith("t_s"):
                continue
            # ì‰¼í‘œë¡œ êµ¬ë¶„
            parts = line.split(",")
            try:
                timestamps.append(float(parts[0]))  # ì²« ë²ˆì§¸ ì—´ì´ íƒ€ì„ìŠ¤íƒ¬í”„
            except (ValueError, IndexError):
                continue
    return np.array(timestamps, dtype=np.float32) if timestamps else None

def create_bpm_labels(gt_times, z_tau, window_sec=3.0):
    """ECG íƒ€ì„ìŠ¤íƒ¬í”„ë¡œë¶€í„° êµ¬ê°„ì˜ í‰ê·  BPM ë¼ë²¨ ìƒì„±
    
    IBI(Inter-Beat Interval) ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ BPM ê³„ì‚°:
    - êµ¬ê°„ ë‚´ ë¹„íŠ¸ 2ê°œ ì´ìƒ: IBI í‰ê· ìœ¼ë¡œ BPM ê³„ì‚° (60 / mean_IBI)
    """
    T = len(z_tau); labels = []
    for i in range(WIN_FRAMES, T, HOP_FRAMES):
        t_end = i / FS; t_start = max(0.0, t_end - window_sec)
        # ì°½ ì•ˆ ë¹„íŠ¸
        m = (gt_times >= t_start) & (gt_times < t_end)
        beats = gt_times[m]
        bpm = 60.0
        if len(beats) >= 2:
            ibi = np.diff(beats)          # ì°½ ë‚´ë¶€ IBIë“¤
            bpm = 60.0 / float(np.mean(ibi))
        elif len(beats) == 1:
            # bpm = 0
            raise ValueError("ë¹„íŠ¸ê°€ 1ê°œ ì´í•˜ì…ë‹ˆë‹¤.")
        labels.append(bpm)  # ì •ê·œí™” ì—†ì´ ì§ì ‘ BPM ê°’ ì‚¬ìš©
    return np.array(labels, dtype=np.float32)

# ===== ë°ì´í„° ìƒì„± í•¨ìˆ˜ =====
def create_training_data(all_z_tau: List[np.ndarray], all_gt_times: List[np.ndarray], predictor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """ëª¨ë“  íŒŒì¼ì—ì„œ í•™ìŠµ/ê²€ì¦ ë°ì´í„° ìƒì„± (íŒŒì¼ ë‹¨ìœ„ ë¶„í• )"""
    print("í•™ìŠµ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # íŒŒì¼ë³„ë¡œ íŠ¹ì§•ê³¼ ë¼ë²¨ ìˆ˜ì§‘
    file_features = []
    file_labels = []
    
    for z_tau, gt_times in zip(all_z_tau, all_gt_times):
        bpm_labels = create_bpm_labels(gt_times, z_tau)
        file_feature_list = []
        file_label_list = []
        window_idx = 0
        
        for i in range(WIN_FRAMES, len(z_tau), HOP_FRAMES):
            if window_idx >= len(bpm_labels):
                break
                
            window = z_tau[i-WIN_FRAMES:i]
            # ê¸°ì¡´ì˜ process_window ë©”ì„œë“œ ì¬ì‚¬ìš©
            window_features = predictor.process_window(window)
            
            file_feature_list.append(window_features)
            file_label_list.append(bpm_labels[window_idx])
            window_idx += 1
        
        # íŒŒì¼ë³„ë¡œ íŠ¹ì§•ê³¼ ë¼ë²¨ ì €ì¥
        if file_feature_list:  # ë¹ˆ íŒŒì¼ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
            file_features.append(np.array(file_feature_list, dtype=np.float32))
            file_labels.append(np.array(file_label_list, dtype=np.float32))
    
    print(f"ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(file_features)}ê°œ íŒŒì¼")
    
    # êµ¬ê°„ë§ˆë‹¤ ê³¨ê³ ë£¨ ëœë¤ ì„ íƒìœ¼ë¡œ ê²€ì¦ ë°ì´í„° ë¶„í• 
    num_files = len(file_features)
    num_val_files = max(1, int(round(num_files * VALIDATION_SPLIT)))
    num_val_files = min(num_val_files, num_files)
    
    # 20ê°œ ë‹¨ìœ„ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê° êµ¬ê°„ì—ì„œ ëœë¤í•˜ê²Œ ì„ íƒ (1~20, 21~40, 41~60, ...)
    bucket_size = 20
    val_indices = []
    
    # ê° êµ¬ê°„ì—ì„œ ì„ íƒí•  ê°œìˆ˜ ê³„ì‚° (ë¼ìš´ë“œë¡œë¹ˆ)
    num_buckets = (num_files + bucket_size - 1) // bucket_size  # ì˜¬ë¦¼ ê³„ì‚°
    selections_per_bucket = [0] * num_buckets
    
    # í•„ìš”í•œ ê²€ì¦ íŒŒì¼ ìˆ˜ë¥¼ êµ¬ê°„ì— ê³¨ê³ ë£¨ ë¶„ë°°
    for i in range(num_val_files):
        bucket_idx = i % num_buckets
        selections_per_bucket[bucket_idx] += 1
    
    # ê° êµ¬ê°„ì—ì„œ ëœë¤í•˜ê²Œ ì„ íƒ
    rng = np.random.default_rng(42)  # ì¬í˜„ì„±ì„ ìœ„í•´ ë³„ë„ì˜ Generator ì‚¬ìš©
    for bucket_idx in range(num_buckets):
        start_idx = bucket_idx * bucket_size
        end_idx = min(start_idx + bucket_size, num_files)
        bucket_files = list(range(start_idx, end_idx))
        
        # ì´ êµ¬ê°„ì—ì„œ ì„ íƒí•  ê°œìˆ˜ë§Œí¼ ëœë¤ ìƒ˜í”Œë§
        num_selections = selections_per_bucket[bucket_idx]
        if num_selections > 0 and len(bucket_files) > 0:
            selected = rng.choice(bucket_files,
                                      size=min(num_selections, len(bucket_files)), 
                                      replace=False)
            val_indices.extend(selected)
    
    # í›ˆë ¨ ì¸ë±ìŠ¤ëŠ” ê²€ì¦ ì¸ë±ìŠ¤ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€
    train_indices = [i for i in range(num_files) if i not in set(val_indices)]
    
    # í›ˆë ¨ ë°ì´í„° ê²°í•©
    train_features = []
    train_labels = []
    for idx in train_indices:
        train_features.extend(file_features[idx])
        train_labels.extend(file_labels[idx])
    
    # ê²€ì¦ ë°ì´í„° ê²°í•©
    val_features = []
    val_labels = []
    for idx in val_indices:
        val_features.extend(file_features[idx])
        val_labels.extend(file_labels[idx])
    
    # numpy ë°°ì—´ë¡œ ë³€í™˜
    X_train = np.array(train_features, dtype=np.float32)
    y_train = np.array(train_labels, dtype=np.float32)
    X_val = np.array(val_features, dtype=np.float32)
    y_val = np.array(val_labels, dtype=np.float32)

    # ë¼ë²¨ ì •ê·œí™” í†µê³„ (í›ˆë ¨ ì„¸íŠ¸ ê¸°ì¤€)
    predictor.label_mean = float(np.mean(y_train)) if len(y_train) > 0 else 0.0
    predictor.label_std = float(np.std(y_train) + 1e-6) if len(y_train) > 0 else 1.0
    print(f"ë¼ë²¨ ì •ê·œí™” í†µê³„: mean={predictor.label_mean:.2f}, std={predictor.label_std:.2f}")

    # ë¼ë²¨ í‘œì¤€í™”
    if predictor.label_std > 0:
        y_train = (y_train - predictor.label_mean) / predictor.label_std
        y_val = (y_val - predictor.label_mean) / predictor.label_std
    
    print(f"íŒŒì¼ ë‹¨ìœ„ ë°ì´í„° ë¶„í•  ì™„ë£Œ (êµ¬ê°„ë³„ ê³¨ê³ ë£¨ ëœë¤ ìƒ˜í”Œë§):")
    print(f"  ê²€ì¦ íŒŒì¼ ì¸ë±ìŠ¤: {sorted(val_indices)}")
    print(f"  í›ˆë ¨ íŒŒì¼: {len(train_indices)}ê°œ, ìœˆë„ìš°: {len(X_train)}ê°œ")
    print(f"  ê²€ì¦ íŒŒì¼: {len(val_indices)}ê°œ, ìœˆë„ìš°: {len(X_val)}ê°œ")
    
    return (torch.from_numpy(X_train), torch.from_numpy(y_train), 
            torch.from_numpy(X_val), torch.from_numpy(y_val))

# ===== 1D CNN + LSTM BPM íšŒê·€ ëª¨ë¸ ì •ì˜ =====
class BPMRegressionModel(nn.Module):
    """BPM ì˜ˆì¸¡ì„ ìœ„í•œ 1D CNN + LSTM íšŒê·€ ëª¨ë¸"""
    
    def __init__(self, input_dim: int = FEATURE_DIM, hidden: int = HIDDEN_DIM, num_layers: int = NUM_LAYERS):
        super().__init__()
        
        # 1D CNN: ì£¼íŒŒìˆ˜ íŠ¹ì§• ì••ì¶• ë° ì¶”ì¶œ (LNì„ CNN ëë‹¨ì— ì ìš©)
        self.conv1d = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            # nn.BatchNorm1d(64),
            # nn.Conv1d(64, 64, kernel_size=3, padding=1),  # ì¶œë ¥ ì±„ë„ì„ 64ë¡œ ì¦ê°€
            # nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool1d(1),   # (N,64,1)
            nn.Flatten(1),              # (N,64)
            nn.LayerNorm(64),   # â˜… í•œ ì¤„
        )
        
        # LSTM: ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ
        self.lstm = nn.LSTM(
            input_size=64,  # CNN ì¶œë ¥ ì°¨ì› 
            hidden_size=hidden,
            num_layers=num_layers,
            # dropout=0.3,   # ì¸µ ì‚¬ì´ ë“œë¡­ì•„ì›ƒ (num_layers>1ì—ì„œë§Œ ì‘ë™)
            batch_first=True
        )
        
        # íšŒê·€ í—¤ë“œ (BPM ì˜ˆì¸¡) - ì§ì ‘ BPM ê°’ ì¶œë ¥ (30-200)
        self.regressor = nn.Sequential(
            nn.Linear(hidden, hidden // 4),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden // 4, hidden // 16),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden // 16, 1),  # BPM ê°’ ì§ì ‘ ì¶œë ¥
        )
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
        
        # ë§ˆì§€ë§‰ ë ˆì´ì–´ë¥¼ ì ì ˆí•œ ë²”ìœ„ë¡œ ì¬ì´ˆê¸°í™”
        with torch.no_grad():
            self.regressor[-1].weight.normal_(0, 0.05)
            self.regressor[-1].bias.zero_()
        
        # ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
        print(f"BPM íšŒê·€ ëª¨ë¸ êµ¬ì¡°:")
        print(f"  1D CNN: {input_dim} -> 64")
        print(f"  LSTM: 64 -> {hidden} (layers={num_layers})")
        print(f"  Regressor: {hidden} -> 1 (ì§ì ‘ BPM ì¶œë ¥)")
        
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Conv1d):
                nn.init.kaiming_normal_(module.weight, nonlinearity='relu')
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight' in name:
                        nn.init.xavier_uniform_(param)
                    elif 'bias' in name:
                        nn.init.zeros_(param)
    
    def _calculate_train_labels_mean(self) -> float:
        """í›ˆë ¨ ë¼ë²¨ì˜ í‰ê· ê°’ì„ ê³„ì‚°í•˜ì—¬ ë°”ì´ì–´ìŠ¤ ì´ˆê¸°í™”ì— ì‚¬ìš©"""
        try:
            # í›ˆë ¨ ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ì •ë‹µ íŒŒì¼ì„ ì½ì–´ì„œ BPM í‰ê·  ê³„ì‚°
            train_answer_dir = TRAIN_ANSWER_DIR
            if not os.path.exists(train_answer_dir):
                print("[WARNING] í›ˆë ¨ ì •ë‹µ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 80.0 ì‚¬ìš©")
                return 80.0
            
            all_bpms = []
            answer_files = [f for f in os.listdir(train_answer_dir) if f.endswith('.csv')]
            
            for answer_file in answer_files:
                answer_path = os.path.join(train_answer_dir, answer_file)
                try:
                    gt_times = load_ground_truth(answer_path)
                    if gt_times is not None and len(gt_times) > 1:
                        # ì‹œê°„ ê°„ê²©ìœ¼ë¡œ BPM ê³„ì‚°
                        time_span = gt_times[-1] - gt_times[0]
                        if time_span > 0:
                            bpm = 60.0 * (len(gt_times) - 1) / time_span
                            if 30 <= bpm <= 200:  # ìœ íš¨í•œ BPM ë²”ìœ„
                                all_bpms.append(bpm)
                except Exception as e:
                    print(f"[WARNING] {answer_file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            if all_bpms:
                mean_bpm = np.mean(all_bpms)
                print(f"[INFO] í›ˆë ¨ ë¼ë²¨ í‰ê·  ê³„ì‚° ì™„ë£Œ: {len(all_bpms)}ê°œ íŒŒì¼, í‰ê·  BPM: {mean_bpm:.2f}")
                return float(mean_bpm)
            else:
                print("[WARNING] ìœ íš¨í•œ BPM ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 80.0 ì‚¬ìš©")
                return 80.0
                
        except Exception as e:
            print(f"[WARNING] í›ˆë ¨ ë¼ë²¨ í‰ê·  ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}. ê¸°ë³¸ê°’ 80.0 ì‚¬ìš©")
            return 80.0
        
    def forward(self, x: torch.Tensor, hidden: Optional[Tuple] = None) -> Tuple[torch.Tensor, Tuple]:
        """
        Args:
            x: (batch, seq_len, feat_dim)
            hidden: LSTM hidden state
        Returns:
            bpm_pred: (batch, 1) - ì˜ˆì¸¡ëœ BPM ê°’
            hidden: ì—…ë°ì´íŠ¸ëœ hidden state
        """
        # # x: (B, T, F)
        # mu  = x.mean(dim=1, keepdim=True)                 # (B, 1, F)  ì‹œê°„ í‰ê· 
        # std = x.std(dim=1, keepdim=True) + 1e-6           # (B, 1, F)
        # x = (x - mu) / std                                # (B, T, F)

        batch_size, seq_len, feat_dim = x.shape
        
        # 1D CNN ì ìš©ì„ ìœ„í•´ ì°¨ì› ì¬ë°°ì—´: (batch, seq_len, feat_dim) -> (batch * seq_len, 1, feat_dim)
        x_reshaped = x.view(-1, 1, feat_dim)
        
        # 1D CNN íŠ¹ì§• ì¶”ì¶œ: (batch * seq_len, 1, feat_dim) -> (batch * seq_len, 64, 1)
        conv_out = self.conv1d(x_reshaped)
        
        # LSTM ì…ë ¥ì„ ìœ„í•´ ì°¨ì› ì¬ë°°ì—´: (batch * seq_len, 64, 1) -> (batch, seq_len, 64)
        conv_out = conv_out.squeeze(-1).view(batch_size, seq_len, 64)
        
        # LSTM ì²˜ë¦¬
        lstm_out, hidden = self.lstm(conv_out, hidden)
        # lstm_out, hidden = self.lstm(x, hidden)

        # ë§ˆì§€ë§‰ ì‹œì ì˜ ì¶œë ¥ë§Œ ì‚¬ìš© (ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ ê³ ë ¤í•œ BPM ì˜ˆì¸¡)
        last_output = lstm_out[:, -1, :]  # (batch, hidden_dim)
        # last_output = lstm_out.mean(dim=1)  # (batch, hidden_dim)
        
        # ì§ì ‘ BPM íšŒê·€ (ì„ í˜• ì¶œë ¥, í´ë¦¬í•‘ ì œê±°)
        bpm_pred = self.regressor(last_output)  # (batch, 1)
        
        # í´ë¦¬í•‘ì„ ì œê±°í•˜ì—¬ ëª¨ë¸ì´ ììœ ë¡­ê²Œ í•™ìŠµí•˜ë„ë¡ í•¨
        # ì†ì‹¤ ê³„ì‚°ì—ì„œë§Œ ë²”ìœ„ ì œí•œ ì ìš©
        
        return bpm_pred, hidden

# ===== BPM ì˜ˆì¸¡ ì‹œìŠ¤í…œ =====
class BPMPredictor:
    """ë””ë ‰í„°ë¦¬ ê¸°ë°˜ BPM íšŒê·€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""
    
    def __init__(self, train_data_dir: str, train_answer_dir: str):
        """
        Args:
            train_data_dir: í•™ìŠµ ë°ì´í„° í´ë” ê²½ë¡œ
            train_answer_dir: í•™ìŠµ ì •ë‹µ í´ë” ê²½ë¡œ
        """
        self.train_data_dir = train_data_dir
        self.train_answer_dir = train_answer_dir
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # ë¼ë²¨ ì •ê·œí™” í†µê³„ (í›ˆë ¨ ì‹œ ê³„ì‚°)
        self.label_mean: Optional[float] = None
        self.label_std: Optional[float] = None
        print(f"ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"í•™ìŠµ ë°ì´í„° ê²½ë¡œ: {train_data_dir}")
        print(f"í•™ìŠµ ì •ë‹µ ê²½ë¡œ: {train_answer_dir}")
        
    def load_all_training_data(self) -> Tuple[List[np.ndarray], List[np.ndarray]]:
        """ëª¨ë“  í•™ìŠµ íŒŒì¼ ë¡œë”©"""
        pairs = find_matching_files(self.train_data_dir, self.train_answer_dir)
        
        if not pairs:
            raise FileNotFoundError("í•™ìŠµ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"=== ë‹¤ì¤‘ íŒŒì¼ ë°ì´í„° ë¡œë”© ===")
        print(f"í•™ìŠµ ë°ì´í„° {len(pairs)}ê°œ íŒŒì¼ ë°œê²¬:")
        all_z_tau = []
        all_gt_times = []
        
        for i, (data_path, answer_path) in enumerate(pairs):
            file_num = os.path.splitext(os.path.basename(data_path))[0]
            print(f"  {i+1}. íŒŒì¼ {file_num}: {data_path}")
            
            # ë°ì´í„° ë¡œë”©
            fc_bin, z_tau = calculation(data_path)
            gt_times = load_ground_truth(answer_path)
            
            if gt_times is not None:
                all_z_tau.append(z_tau)
                all_gt_times.append(gt_times)
                avg_bpm = 60.0 * len(gt_times) / (gt_times[-1] - gt_times[0])
                print(f"     - ê±°ë¦¬ bin: {fc_bin}, ë¹„íŠ¸: {len(gt_times)}ê°œ, í‰ê·  BPM: {avg_bpm:.1f}")
            else:
                print(f"     - âš ï¸ ì •ë‹µ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨")
        
        print(f"ì´ {len(all_z_tau)}ê°œ íŒŒì¼ ë¡œë”© ì™„ë£Œ")
        
        # BPM ë¶„í¬ ë¶„ì„
        all_bpms = []
        for gt_times in all_gt_times:
            if len(gt_times) > 1:
                avg_bpm = 60.0 * len(gt_times) / (gt_times[-1] - gt_times[0])
                all_bpms.append(avg_bpm)
        
        if all_bpms:
            print(f"\n=== í•™ìŠµ ë°ì´í„° BPM ë¶„í¬ ===")
            print(f"í‰ê· : {np.mean(all_bpms):.1f} BPM")
            print(f"ë²”ìœ„: {np.min(all_bpms):.1f} - {np.max(all_bpms):.1f} BPM")
            print(f"í‘œì¤€í¸ì°¨: {np.std(all_bpms):.1f} BPM")
            
        return all_z_tau, all_gt_times
    
    def process_window(self, window: np.ndarray) -> np.ndarray:
        """8ì´ˆ ìœˆë„ìš°ë¥¼ 8ê°œ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìœ„ìƒë¯¸ë¶„ ì‹ í˜¸ë¥¼ ì§ì ‘ ì‚¬ìš© (FFT ì—†ìŒ)"""
        # 8ì´ˆ ìœˆë„ìš°ë¥¼ 8ê°œ ì„œë¸Œêµ¬ê°„ìœ¼ë¡œ ë¶„í•  (ê° 1ì´ˆ, 36í”„ë ˆì„)
        sub_intervals = WIN_FRAMES // HOP_FRAMES
        sub_window_size = HOP_FRAMES
        sub_features = []
        
        for i in range(sub_intervals):
            start_idx = i * sub_window_size
            end_idx = (i + 1) * sub_window_size if i < sub_intervals - 1 else len(window)
            
            # ì„œë¸Œìœˆë„ìš° ì¶”ì¶œ
            sub_window = window[start_idx:end_idx]
            
            # ì„œë¸Œìœˆë„ìš°ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì´ì „ íŠ¹ì§• ì¬ì‚¬ìš©
            if len(sub_window) < 5:  # ìµœì†Œ 5í”„ë ˆì„ í•„ìš”
                print(f"ì„œë¸Œìœˆë„ìš° í¬ê¸° ë¶€ì¡±: {len(sub_window)}")
                raise ValueError(f"ì„œë¸Œìœˆë„ìš° í¬ê¸° ë¶€ì¡±: {len(sub_window)}")
            
            # ìœ„ìƒë¯¸ë¶„ ì‹ í˜¸ë¥¼ FFT ì—†ì´ ì§ì ‘ ì‚¬ìš©
            try:
                # ìœ„ìƒë¯¸ë¶„ ì¶”ì¶œ (ì‹œê°„ ì˜ì—­ ì‹ í˜¸, 13í”„ë ˆì„) - BPF ì—†ìŒ
                dphi = extract_phase_derivative(sub_window, FS)
                
                # ì°¨ì› ë§ì¶¤: dphië¥¼ FEATURE_DIMìœ¼ë¡œ ì¡°ì •
                if len(dphi) >= FEATURE_DIM:
                    # ë‹¤ìš´ìƒ˜í”Œë§ìœ¼ë¡œ ì••ì¶•
                    step = len(dphi) / FEATURE_DIM
                    compressed = np.zeros(FEATURE_DIM, dtype=np.float32)
                    for j in range(FEATURE_DIM):
                        start_feat = int(j * step)
                        end_feat = int((j + 1) * step)
                        if end_feat > len(dphi):
                            end_feat = len(dphi)
                        if start_feat < end_feat:
                            compressed[j] = np.mean(dphi[start_feat:end_feat])
                    sub_features.append(compressed)
                else:
                    # ì œë¡œíŒ¨ë”©ìœ¼ë¡œ ì°¨ì› í™•ì¥
                    result = np.zeros(FEATURE_DIM, dtype=np.float32)
                    result[:len(dphi)] = dphi
                    sub_features.append(result)
            except Exception as e:
                print(f"ì—ëŸ¬ ë°œìƒ: {e}")
                raise e
        
        # # 16ê°œ ì„œë¸Œêµ¬ê°„ì„ í•˜ë‚˜ì˜ ë°°ì—´ë¡œ ê²°í•©
        # return np.array(sub_features, dtype=np.float32)  # shape: (16, FEATURE_DIM)
         # (8, FEATURE_DIM)ìœ¼ë¡œ ê²°í•©
        feats = np.array(sub_features, dtype=np.float32)  # (8, 36)
        # ì„œë¸Œêµ¬ê°„(í–‰)ë³„ z-score ì •ê·œí™” ì œê±°: í¬ê¸°/ìŠ¤ì¼€ì¼ ì •ë³´ ë³´ì¡´
        return feats
    

    
    def train_on_multiple_files(self, all_z_tau: List[np.ndarray], all_gt_times: List[np.ndarray], batch_size: int = 32):
        """ì—¬ëŸ¬ íŒŒì¼ë¡œ DataLoader ë°°ì¹˜ í•™ìŠµ (ê²€ì¦ ë° ì–¼ë¦¬ ìŠ¤íƒ‘ í¬í•¨)"""
        first_features = self.process_window(all_z_tau[0][:WIN_FRAMES])
        print(f"íŠ¹ì§• ì°¨ì›: {first_features.shape}")
        
        self.model = BPMRegressionModel(input_dim=FEATURE_DIM).to(self.device)
        print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # íŒŒë¼ë¯¸í„° ê·¸ë£¹: ë§ˆì§€ë§‰ íšŒê·€ ë ˆì´ì–´ì— ë” í° í•™ìŠµë¥  (í•­ë“±ì„±ìœ¼ë¡œ ë¶„ë¦¬)
        last_layer_params = list(self.model.regressor[-1].parameters())
        last_param_ids = {id(p) for p in last_layer_params}
        other_params = [p for p in self.model.parameters() if id(p) not in last_param_ids]
        optimizer = torch.optim.Adam([
            {"params": other_params, "lr": LEARNING_RATE},
            {"params": last_layer_params, "lr": LEARNING_RATE * 5.0},
        ])
        
        # í•™ìŠµ/ê²€ì¦ ë°ì´í„° ìƒì„± ë° DataLoader ì„¤ì •
        X_train, y_train, X_val, y_val = create_training_data(all_z_tau, all_gt_times, self)
        
        # ë¼ë²¨ ì •ê·œí™” í†µê³„ ê³„ì‚° í›„ Huber Lossì˜ beta ì¡°ì •
        beta_bpm = 3.0                             # 2~4 BPM ì¤‘ í•˜ë‚˜ë¡œ íŠœë‹
        beta_std = beta_bpm / self.label_std        # z-ìŠ¤ì¼€ì¼ ì„ê³„ì¹˜
        
        criterion = torch.nn.SmoothL1Loss(beta=beta_std)
        print(f"Huber Loss beta ì¡°ì •: {beta_bpm} BPM -> {beta_std:.3f} (z-scale)")
        # criterion = torch.nn.SmoothL1Loss(beta=4.0)  # Huber loss
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€ (ê²€ì¦ ì†ì‹¤ ê¸°ë°˜)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',           # ê²€ì¦ ì†ì‹¤ ìµœì†Œí™”
            factor=SCHEDULER_FACTOR,  # í•™ìŠµë¥  ê°ì†Œ ë¹„ìœ¨
            patience=SCHEDULER_PATIENCE,  # ì¸ë‚´ì‹¬
            min_lr=SCHEDULER_MIN_LR,     # ìµœì†Œ í•™ìŠµë¥ 
        )

        print(f"\n=== ë°°ì¹˜ í•™ìŠµ ì‹œì‘ (ì—í¬í¬: {EPOCHS}, ë°°ì¹˜í¬ê¸°: {batch_size}) ===")
        print(f"ê²€ì¦ ë°ì´í„° ë¹„ìœ¨: {VALIDATION_SPLIT*100:.0f}%, ì–¼ë¦¬ ìŠ¤íƒ‘ ì¸ë‚´ì‹¬: {EARLY_STOP_PATIENCE} ì—í¬í¬")
        print(f"ìŠ¤ì¼€ì¤„ëŸ¬: ReduceLROnPlateau (factor={SCHEDULER_FACTOR}, patience={SCHEDULER_PATIENCE})")
        
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)
        
        train_dataloader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,  # ì—í¬í¬ë§ˆë‹¤ ìë™ ì…”í”Œë§
            num_workers=0,  # Windowsì—ì„œ ì•ˆì •ì 
            pin_memory=torch.cuda.is_available(),
            drop_last=False # ë§ˆì§€ë§‰ ë°°ì¹˜ ë²„ë¦¬ì§€ ì•ŠìŒ
        )
        
        val_dataloader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,  # ê²€ì¦ì€ ì…”í”Œ ë¶ˆí•„ìš”
            num_workers=0,
            pin_memory=torch.cuda.is_available(),
            drop_last=False
        )
        
        print(f"DataLoader ìƒì„±: í›ˆë ¨ {len(train_dataset)}ê°œ, ê²€ì¦ {len(val_dataset)}ê°œ ìœˆë„ìš°")
        
        # ì–¼ë¦¬ ìŠ¤íƒ‘ ë³€ìˆ˜ë“¤
        best_val_loss = float('inf')
        patience_counter = 0
        best_model_state = None
        
        for epoch in range(EPOCHS):
            # í›ˆë ¨ ë‹¨ê³„
            self.model.train()
            train_loss = train_mae = batch_count = 0
            
            for features, labels in train_dataloader:
                features = features.to(self.device)
                labels = labels.squeeze().to(self.device)
                
                optimizer.zero_grad()
                bpm_pred, _ = self.model(features, None)
                
                # loss = F.mse_loss(bpm_pred.squeeze(), labels)
                loss = criterion(bpm_pred.squeeze(), labels) # Huber loss (ì •ê·œí™” ë¼ë²¨ ê¸°ì¤€)
                # MAEëŠ” BPM ë‹¨ìœ„ë¡œ ê³„ì‚° (ì—­ì •ê·œí™”)
                if (self.label_mean is not None) and (self.label_std is not None):
                    pred_bpm = bpm_pred.squeeze() * self.label_std + self.label_mean
                    true_bpm = labels * self.label_std + self.label_mean
                    mae = F.l1_loss(pred_bpm, true_bpm)
                else:
                    mae = F.l1_loss(bpm_pred.squeeze(), labels)
                
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                train_mae += mae.item()
                batch_count += 1
            
            avg_train_loss = train_loss / max(1, batch_count)
            avg_train_mae = train_mae / max(1, batch_count)
            
            # ê²€ì¦ ë‹¨ê³„
            self.model.eval()
            val_loss = val_mae = val_batch_count = 0
            
            with torch.no_grad():
                for features, labels in val_dataloader:
                    features = features.to(self.device)
                    labels = labels.squeeze().to(self.device)
                    
                    bpm_pred, _ = self.model(features, None)
                    
                    # í›ˆë ¨ê³¼ ë™ì¼í•œ Huber Loss ì‚¬ìš© (ì •ê·œí™” ë¼ë²¨ ê¸°ì¤€)
                    loss = criterion(bpm_pred.squeeze(), labels)
                    # MAEëŠ” BPM ë‹¨ìœ„ë¡œ ê³„ì‚° (ì—­ì •ê·œí™”)
                    if (self.label_mean is not None) and (self.label_std is not None):
                        pred_bpm = bpm_pred.squeeze() * self.label_std + self.label_mean
                        true_bpm = labels * self.label_std + self.label_mean
                        mae = F.l1_loss(pred_bpm, true_bpm)
                    else:
                        mae = F.l1_loss(bpm_pred.squeeze(), labels)
                    
                    val_loss += loss.item()
                    val_mae += mae.item()
                    val_batch_count += 1
            
            avg_val_loss = val_loss / max(1, val_batch_count)
            avg_val_mae = val_mae / max(1, val_batch_count)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í… (ê²€ì¦ ì†ì‹¤ ê¸°ë°˜)
            old_lr = optimizer.param_groups[0]['lr']
            scheduler.step(avg_val_loss)
            current_lr = optimizer.param_groups[0]['lr']
            
            # í•™ìŠµë¥  ë³€ê²½ ê°ì§€ ë° ì¶œë ¥
            if current_lr != old_lr:
                print(f"ğŸ”½ í•™ìŠµë¥  ê°ì†Œ: {old_lr:.2e} â†’ {current_lr:.2e}")
            
            # ì–¼ë¦¬ ìŠ¤íƒ‘ ì²´í¬
            if avg_val_loss < best_val_loss - EARLY_STOP_MIN_DELTA:
                best_val_loss = avg_val_loss
                patience_counter = 0
                best_model_state = self.model.state_dict().copy()
                print(f"Epoch {epoch+1}/{EPOCHS} | í›ˆë ¨ Loss: {avg_train_loss:.6f} MAE: {avg_train_mae:.4f} | ê²€ì¦ Loss: {avg_val_loss:.6f} MAE: {avg_val_mae:.4f} | LR: {current_lr:.2e} | ğŸ¯ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥!")
            else:
                patience_counter += 1
                if (epoch + 1) % 50 == 0:
                    print(f"Epoch {epoch+1}/{EPOCHS} | í›ˆë ¨ Loss: {avg_train_loss:.6f} MAE: {avg_train_mae:.4f} | ê²€ì¦ Loss: {avg_val_loss:.6f} MAE: {avg_val_mae:.4f} | LR: {current_lr:.2e} | ì¸ë‚´ì‹¬: {patience_counter}/{EARLY_STOP_PATIENCE}")
            
            # ì–¼ë¦¬ ìŠ¤íƒ‘ ì¡°ê±´ í™•ì¸
            if patience_counter >= EARLY_STOP_PATIENCE:
                print(f"\n\n!!! ì–¼ë¦¬ ìŠ¤íƒ‘! {EARLY_STOP_PATIENCE} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
                print(f"ìµœê³  ê²€ì¦ Loss: {best_val_loss:.6f} (Epoch {epoch+1-EARLY_STOP_PATIENCE})")
                break
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ë³µì›
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
            print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ë³µì› ì™„ë£Œ (ê²€ì¦ Loss: {best_val_loss:.6f})")
        
        # ëª¨ë¸ ì €ì¥
        os.makedirs("checkpoints", exist_ok=True)
        torch.save(self.model.state_dict(), "checkpoints/bpm_regressor.pt")
        print("\nëª¨ë¸ ì €ì¥ ì™„ë£Œ: checkpoints/bpm_regressor.pt")
    
    def _predict_windows(self, z_tau: np.ndarray) -> List[float]:
        """ìœˆë„ìš°ë³„ BPM ì˜ˆì¸¡ (ê³µí†µ í•¨ìˆ˜)"""
        self.model.eval()
        predictions = []
        
        with torch.no_grad():
            for i in range(WIN_FRAMES, len(z_tau), HOP_FRAMES):
                window = z_tau[i-WIN_FRAMES:i]
                features = self.process_window(window)
                x = torch.FloatTensor(features).unsqueeze(0).to(self.device)
                
                bpm_pred, _ = self.model(x, None)
                y = bpm_pred.cpu().item()
                # ì—­ì •ê·œí™”(ë¼ë²¨ ì •ê·œí™”ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°)
                if (self.label_mean is not None) and (self.label_std is not None):
                    y = y * self.label_std + self.label_mean
                predictions.append(y)
                
        return predictions
    
    def predict_bpm(self, z_tau: np.ndarray) -> Tuple[List[float], List[float]]:
        """BPM ì˜ˆì¸¡ (ì‹œê°„ ì •ë³´ í¬í•¨)"""
        predictions = self._predict_windows(z_tau)
        times = [i / FS for i in range(WIN_FRAMES, len(z_tau), HOP_FRAMES)][:len(predictions)]
        return predictions, times
    
    def calculate_epoch_metrics(self, z_tau: np.ndarray, gt_times: np.ndarray) -> Dict:
        """ì—í¬í¬ë³„ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        
        # ì˜ˆì¸¡ BPM
        pred_bpms = self._predict_windows(z_tau)
        
        # ì •ë‹µ BPM (ì´ë¯¸ ì‹¤ì œ BPM ê°’)
        true_bpms = create_bpm_labels(gt_times, z_tau)
        
        if len(pred_bpms) == 0 or len(true_bpms) == 0:
            return {"rmse": 0.0, "mae": 0.0, "avg_pred_bpm": 0.0, "avg_true_bpm": 0.0}
        
        # ê¸¸ì´ ë§ì¶¤ ë° ë©”íŠ¸ë¦­ ê³„ì‚°
        min_len = min(len(pred_bpms), len(true_bpms))
        pred_array = np.array(pred_bpms[:min_len])
        true_array = np.array(true_bpms[:min_len])
        
        return {
            "rmse": float(np.sqrt(np.mean((pred_array - true_array) ** 2))),
            "mae": float(np.mean(np.abs(pred_array - true_array))),
            "avg_pred_bpm": float(np.mean(pred_array)),
            "avg_true_bpm": float(np.mean(true_array))
        }
    
    def evaluate_predictions(self, z_tau: np.ndarray, gt_times: np.ndarray) -> None:
        """ì˜ˆì¸¡ ì„±ëŠ¥ì„ êµ¬ê°„ë³„ë¡œ ìƒì„¸ í‰ê°€ (í˜„ì¬ ì‚¬ìš© ì•ˆí•¨)"""
 
        pred_bpms, window_times = self.predict_bpm(z_tau)
        true_bpms = create_bpm_labels(gt_times, z_tau)
        
        # ê¸¸ì´ ë§ì¶¤
        min_len = min(len(pred_bpms), len(true_bpms))
        pred_bpms = pred_bpms[:min_len]
        true_bpms = true_bpms[:min_len]
        window_times = window_times[:min_len]
        
        print(f"\n=== êµ¬ê°„ë³„ BPM ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€ ===")
        
        # ì›í•˜ëŠ” êµ¬ê°„ ê¸¸ì´(ì´ˆ)ì™€ í˜„ì¬ í™‰ ê°„ê²©ìœ¼ë¡œë¶€í„° êµ¬ê°„ ë‹¹ ìœˆë„ìš° ê°œìˆ˜ ê³„ì‚°
        target_interval_sec = 4
        step_seconds = HOP_FRAMES / FS
        total_duration = len(z_tau) / FS
        interval_size = max(1, int(round(target_interval_sec / step_seconds)))
        for start_idx in range(0, len(pred_bpms), interval_size):
            end_idx = min(start_idx + interval_size, len(pred_bpms))
            
            interval_pred = pred_bpms[start_idx:end_idx]
            interval_true = true_bpms[start_idx:end_idx]
            interval_times = window_times[start_idx:end_idx]
            
            if len(interval_pred) == 0:
                continue
                
            # êµ¬ê°„ ë‚´ í†µê³„
            pred_mean = float(np.mean(interval_pred))
            true_mean = float(np.mean(interval_true))
            pred_std = float(np.std(interval_pred))
            true_std = float(np.std(interval_true))
            rmse = float(np.sqrt(np.mean((np.array(interval_pred) - np.array(interval_true))**2)))
            
            start_time = float(interval_times[0])
            end_time = float(interval_times[-1])
            # ë§ˆì§€ë§‰ êµ¬ê°„ì€ ì‹¤ì œ ì´ ê¸¸ì´ë¥¼ ë°˜ì˜í•´ í‘œì‹œ (ì˜ˆ: 60.0ì´ˆ)
            if end_idx == len(pred_bpms):
                end_time = min(end_time + step_seconds, total_duration)
            
            # í•œ ì¤„ ìš”ì•½ ì¶œë ¥ 
            print(f"êµ¬ê°„ {start_time:.1f}~{end_time:.1f}ì´ˆ | ì˜ˆì¸¡ {pred_mean:.2f}Â±{pred_std:.2f} | ì‹¤ì œ {true_mean:.2f}Â±{true_std:.2f} | RMSE {rmse:.2f} BPM")
            # print()
    
    def test_on_multiple_files(self, test_data_dir: str, test_answer_dir: str):
        """ì—¬ëŸ¬ í…ŒìŠ¤íŠ¸ íŒŒì¼ë¡œ í‰ê°€"""
        test_pairs = find_matching_files(test_data_dir, test_answer_dir)
        
        if not test_pairs:
            print("í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\n=== í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ ({len(test_pairs)}ê°œ íŒŒì¼) ===")
        
        all_rmse = []
        all_mae = []
        
        for i, (test_data_path, test_answer_path) in enumerate(test_pairs):
            file_num = os.path.splitext(os.path.basename(test_data_path))[0]
            print(f"\níŒŒì¼ {file_num} í…ŒìŠ¤íŠ¸:")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì§ì ‘ ë¡œë”©
            fc_bin, test_z_tau = calculation(test_data_path)
            test_gt_times = load_ground_truth(test_answer_path)
            
            if test_gt_times is not None:
                # ì„±ëŠ¥ ê³„ì‚°
                metrics = self.calculate_epoch_metrics(test_z_tau, test_gt_times)
                
                print(f"  RMSE: {metrics['rmse']:.2f} BPM")
                print(f"  MAE: {metrics['mae']:.2f} BPM")
                print(f"  ì˜ˆì¸¡ í‰ê· : {metrics['avg_pred_bpm']:.2f}")
                print(f"  ì‹¤ì œ í‰ê· : {metrics['avg_true_bpm']:.2f}")
                
                # êµ¬ê°„ë³„ ìƒì„¸ í‰ê°€ ì¶”ê°€
                self.evaluate_predictions(test_z_tau, test_gt_times)
                
                all_rmse.append(metrics['rmse'])
                all_mae.append(metrics['mae'])
            else:
                print(f"  âš ï¸ ì •ë‹µ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨")
        
        # ì „ì²´ í‰ê·  ì„±ëŠ¥
        if all_rmse:
            avg_rmse = np.mean(all_rmse)
            avg_mae = np.mean(all_mae)
            std_rmse = np.std(all_rmse)
            std_mae = np.std(all_mae)
            
            print(f"\n=== ì „ì²´ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ===")
            print(f"í‰ê·  RMSE: {avg_rmse:.2f} Â± {std_rmse:.2f} BPM")
            print(f"í‰ê·  MAE: {avg_mae:.2f} Â± {std_mae:.2f} BPM")
            print(f"í…ŒìŠ¤íŠ¸ íŒŒì¼ ìˆ˜: {len(all_rmse)}ê°œ")
    
    def run(self, test_data_dir=None, test_answer_dir=None, batch_size=64):
        """ì „ì²´ ì‹¤í–‰"""
        # 1. ëª¨ë“  í•™ìŠµ ë°ì´í„° ë¡œë”©
        all_z_tau, all_gt_times = self.load_all_training_data()
        
        # 2. ë‹¤ì¤‘ íŒŒì¼ ë°°ì¹˜ í•™ìŠµ
        self.train_on_multiple_files(all_z_tau, all_gt_times, batch_size)
        
        # 3. í…ŒìŠ¤íŠ¸ (ì˜µì…˜)
        if test_data_dir and test_answer_dir:
            self.test_on_multiple_files(test_data_dir, test_answer_dir)
        else:
            print("\nğŸ’¡ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   ì‚¬ìš©ë²•: predictor.run(test_data_dir='path/to/test/', test_answer_dir='path/to/test_answer/')")

if __name__ == "__main__":
    # ë””ë ‰í„°ë¦¬ ê¸°ë°˜ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸
    predictor = BPMPredictor(TRAIN_DATA_DIR, TRAIN_ANSWER_DIR)
    predictor.run(test_data_dir=TEST_DATA_DIR, test_answer_dir=TEST_ANSWER_DIR)
